global: 
  pattern: amd_llm

  clusterDomain: igk.internal

  buildEnabled: false

  amdllm:
    namespace: amd-llm
    build_envs: [] # http_proxy/https_prxy can be set here
    runtime_envs: []

    service_name: chatqna-backend
    service_port: 5008
    container_port: 8888
    docker_file_Path: Dockerfile

    image:
      pre_built: quay.io/sgahlot/opea/chatqna:latest
      from_source: image-registry.openshift-image-registry.svc:5000/opea/chatqna:latest
#
#    git_repo_uri: https://github.com/opea-project/GenAIExamples.git
#    git_ref: 61a8bef   # make sure to validate buildconfig & other change scope before updating


    megaservice_envs:
      - name: RETRIEVER_SERVICE_HOST_IP
        value: retriever.amd-llm.svc.cluster.local
      - name: RETRIEVER_SERVICE_PORT
        value: '"5004"'
      - name: LLM_SERVER_HOST_IP
        value: llm-tgi.amd-llm.svc.cluster.local
      - name: LLM_SERVER_PORT
        value: '"5005"'
      - name: RERANK_SERVER_HOST_IP
        value: tei-reranker-service.amd-llm.svc.cluster.local
      - name: RERANK_SERVER_PORT
        value: '"5006"'
      - name: EMBEDDING_SERVER_HOST_IP
        value: tei-embedding-service.amd-llm.svc.cluster.local
      - name: EMBEDDING_SERVER_PORT
        value: '"5007"'

    env:
      - name: PYTHONPATH
        value: ${PYTHONPATH}:/home/user/GenAIComps
      - name: LOGFLAG
        value: '"True"'
